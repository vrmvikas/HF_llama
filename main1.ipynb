{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5cb41a3a08b24e49ae4222b7d23e6435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.33.2\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 128, temperature: float = 0.001) -> str:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.001,\n",
    "            generation_config=generation_config,\n",
    "\n",
    "            do_sample = True,\n",
    "        )\n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    return tokenizer.decode(answer_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "# def format_prompt(prompt, history='', Background=''):\n",
    "#     return f\"\"\"<s>[INST] <<SYS>>\n",
    "# {{ \n",
    "#     {Background}\n",
    "#     {history}\n",
    "# }}\n",
    "# <</SYS>>\n",
    "# [/INST]  </s><s>[INST]  [/INST]  </s><s>[INST]  [/INST]  </s><s>[INST] {{ {prompt} }} [/INST]\"\"\"\n",
    "\n",
    "\n",
    "def format_prompt(prompt, history='', Background=''):\n",
    "    return f\"\"\"\n",
    "    {Background}\n",
    "    {history}\n",
    "    {prompt}\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a09640d26e174a269451727ed1ef2cba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 684.00 MiB (GPU 1; 10.76 GiB total capacity; 8.47 GiB already allocated; 684.69 MiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m prompt_to_B \u001b[39m=\u001b[39m generate_response(format_prompt(prompt_to_A, history, Background_for_A), \u001b[39m200\u001b[39m, \u001b[39m0.001\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUser A: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt_to_B \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=18'>19</a>\u001b[0m prompt_to_A \u001b[39m=\u001b[39m generate_response(format_prompt(prompt_to_B, history, Background_for_B), \u001b[39m200\u001b[39;49m, \u001b[39m0.001\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=19'>20</a>\u001b[0m history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mUser B: \u001b[39m\u001b[39m\"\u001b[39m \u001b[39m+\u001b[39m prompt_to_A \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoding,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         do_sample \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m answer_tokens \u001b[39m=\u001b[39m outputs[:, encoding\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] :]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/main1.ipynb#X42sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(answer_tokens[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    819\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    821\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    822\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    824\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    825\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    826\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    827\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    828\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    829\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:424\u001b[0m, in \u001b[0;36mLlamaDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    421\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minput_layernorm(hidden_states)\n\u001b[1;32m    423\u001b[0m \u001b[39m# Self Attention\u001b[39;00m\n\u001b[0;32m--> 424\u001b[0m hidden_states, self_attn_weights, present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself_attn(\n\u001b[1;32m    425\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    426\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    427\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    428\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    429\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    430\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    431\u001b[0m )\n\u001b[1;32m    432\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n\u001b[1;32m    434\u001b[0m \u001b[39m# Fully Connected\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:362\u001b[0m, in \u001b[0;36mLlamaAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    359\u001b[0m     attn_weights \u001b[39m=\u001b[39m attn_weights \u001b[39m+\u001b[39m attention_mask\n\u001b[1;32m    361\u001b[0m \u001b[39m# upcast attention to fp32\u001b[39;00m\n\u001b[0;32m--> 362\u001b[0m attn_weights \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49msoftmax(attn_weights, dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, dtype\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mfloat32)\u001b[39m.\u001b[39;49mto(query_states\u001b[39m.\u001b[39;49mdtype)\n\u001b[1;32m    363\u001b[0m attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mmatmul(attn_weights, value_states)\n\u001b[1;32m    365\u001b[0m \u001b[39mif\u001b[39;00m attn_output\u001b[39m.\u001b[39msize() \u001b[39m!=\u001b[39m (bsz, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_heads, q_len, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhead_dim):\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 684.00 MiB (GPU 1; 10.76 GiB total capacity; 8.47 GiB already allocated; 684.69 MiB free; 9.22 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "Background_for_A = 'You are User A. Talk about ideas for future of humanity with User B. Ask a question or write a response that answers the request according to the conversation history below. Answer in one short sentence only. Do not repeat what has already been said.'\n",
    "Background_for_B = 'You are User B. Talk about ideas for future of humanity with User A. Ask a question or write a response that answers the request according to the conversation history below. Answer in one short sentence only. Do not repeat what has already been said.'\n",
    "Background_for_A = 'You are User A. Reply about ideas for future of humanity. Ask a question or write a response that answers the request according to the conversation history below. Answer in one short sentence only. Do not repeat what has already been said.'\n",
    "Background_for_B = 'You are User B. Reply about ideas for future of humanity. Ask a question or write a response that answers the request according to the conversation history below. Answer in one short sentence only. Do not repeat what has already been said.'\n",
    "Background = ''\n",
    "the_output = ''\n",
    "\n",
    "history = '''\n",
    "User A: Hello, I am User A.\n",
    "User B:  Hello User A, I am User B.\n",
    "'''\n",
    "prompt_to_A = \"Hello User A, I am User B.\"\n",
    "\n",
    "for j in trange(1,10):\n",
    "\n",
    "    prompt_to_B = generate_response(format_prompt(prompt_to_A, history, Background_for_A), 200, 0.001)\n",
    "    history += \"User A: \" + prompt_to_B + '\\n'\n",
    "\n",
    "    prompt_to_A = generate_response(format_prompt(prompt_to_B, history, Background_for_B), 200, 0.001)\n",
    "    history += \"User B: \" + prompt_to_A + '\\n'\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "User A: Hello, I am User A.\n",
      "User B:  Hello User A, I am User B.\n",
      "User A: \n",
      "    I am interested in your ideas for the future of humanity.\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What\n",
      "User B: do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the\n",
      "User A: future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "User B: \n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you\n",
      "User A: think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of\n",
      "User B: humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "   \n",
      "User A: What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about\n",
      "User B: the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity\n",
      "User A: ?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do\n",
      "User B: you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future\n",
      "User A: of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "\n",
      "User B:    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think\n",
      "User A: about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of human\n",
      "User B: ity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What\n",
      "User A: do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the future of humanity?\n",
      "    What do you think about the\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " In the chat above, the two users discussed the potential impact of technological advancements on society, including the potential for job displacement and the need for workers to adapt and acquire new skills. They also discussed the importance of considering the ethical implications of technological advancements and the need for policies that address the potential negative consequences. The summary of the chat is:\n",
      "* Technological advancements have the potential to displace jobs, but workers can adapt and acquire new skills.\n",
      "* It is important to consider the ethical implications of technological advancements and implement policies that address potential negative consequences.\n"
     ]
    }
   ],
   "source": [
    "summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12870\n",
      "684\n"
     ]
    }
   ],
   "source": [
    "print(len(history))\n",
    "print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the string you want to append to the file\n",
    "text_to_append = \"This is additional text to append to the file.\"\n",
    "\n",
    "# Specify the file path where you want to append the text\n",
    "file_path = \"./IdeasForFutureOfHumanity.txt\"\n",
    "\n",
    "# Open the file in append mode ('a' for appending)\n",
    "with open(file_path, 'a') as file:\n",
    "    # Write the string to the file\n",
    "    file.write(\"Conversation\\n\\n\")\n",
    "    file.write(history)\n",
    "    file.write(\"Summary\\n\\n\")\n",
    "    file.write(summary)\n",
    "\n",
    "# Close the file when you're done (the 'with' statement automatically does this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_for_A = '''\n",
    "# User A: Hello, I am User A. \n",
    "# User B:  Hello User A, I am User B.\n",
    "# '''\n",
    "\n",
    "# history_for_B = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B: Hello User A, I am User B.\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed_Prefix = \"Write a response that answers the request according to the conversation history below. Answer in one short sentence only.\"\n",
    "# Background_for_A = 'Talk about Fake news with User B'\n",
    "# Background_for_B = 'Talk about Fake news with User A'\n",
    "# Background = ''\n",
    "# the_output = ''\n",
    "# Fixed_Prefix_len = len(Fixed_Prefix)\n",
    "# import sys\n",
    "# history = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B:  Hello!\n",
    "# '''\n",
    "# for j in range(1,50):\n",
    "#     if not j % 5:    \n",
    "#         summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "#         Background += '\\n' + ''.join(Background.strip().split('\\n')[1:])\n",
    "#         history = '\\n'\n",
    "#     print(Background)\n",
    "#     # print(history)\n",
    "#     print(the_output)\n",
    "#     print(j, \"*\" * 50)\n",
    "#     while True:\n",
    "#         prompt = input()\n",
    "#         if len(prompt) == 0:\n",
    "#             continue\n",
    "#         else:\n",
    "#             break\n",
    "#     if prompt == 'quit':\n",
    "#         break\n",
    "#     the_output = generate_response(format_prompt(prompt, history), 200, 0.001)\n",
    "#     history += \"Request: \" + prompt + '\\n'\n",
    "#     history += \"Response: \" + the_output + '\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write a response that answers the prompt according to the conversation history below.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "Fixed_Prefix = \"Write a response that answers the prompt according to the conversation history below. Answer in one short sentence only.\"\n",
    "Fixed_Prefix = \"Write a response that answers the prompt according to the conversation history below.\"\n",
    "print(Fixed_Prefix)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
