{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "526548e9981b4add9bc26e4204c87aa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 4096,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_p\": 0.9,\n",
      "  \"transformers_version\": \"4.33.2\"\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 128, temperature: float = 0.001) -> str:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.001,\n",
    "            generation_config=generation_config,\n",
    "\n",
    "            do_sample = True,\n",
    "        )\n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    return tokenizer.decode(answer_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def format_prompt(prompt, history='', Background=''):\n",
    "    return f\"\"\"<s>[INST] <<SYS>>\n",
    "{{ \n",
    "    {Background}\n",
    "    {history}\n",
    "}}\n",
    "<</SYS>>\n",
    "[/INST]  </s>\n",
    "<s>[INST] {{ {prompt} }} [/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m4\u001b[39m):\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(no_users):\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m         next_prompt \u001b[39m=\u001b[39m generate_response(format_prompt(next_prompt, history, Backgrounds[j]), \u001b[39m150\u001b[39;49m, \u001b[39m0.001\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39mif\u001b[39;00m next_prompt[:\u001b[39mlen\u001b[39m(names[j])] \u001b[39m==\u001b[39m names[j]:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=17'>18</a>\u001b[0m             history \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m next_prompt \u001b[39m+\u001b[39m \u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m'\u001b[39m\n",
      "\u001b[1;32m/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m encoding \u001b[39m=\u001b[39m tokenizer(prompt, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\u001b[39m.\u001b[39mto(model\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mgenerate(\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mencoding,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m         max_new_tokens\u001b[39m=\u001b[39;49mmax_new_tokens,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m         temperature\u001b[39m=\u001b[39;49m\u001b[39m0.001\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m         generation_config\u001b[39m=\u001b[39;49mgeneration_config,\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m         do_sample \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m answer_tokens \u001b[39m=\u001b[39m outputs[:, encoding\u001b[39m.\u001b[39minput_ids\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m] :]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B10.64.34.32/home/suraj/vrmvikas/LLaMA/HF_llama/MultiBot.ipynb#X13sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mdecode(answer_tokens[\u001b[39m0\u001b[39m], skip_special_tokens\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/generation/utils.py:1648\u001b[0m, in \u001b[0;36mGenerationMixin.generate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m   1640\u001b[0m     input_ids, model_kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_expand_inputs_for_generation(\n\u001b[1;32m   1641\u001b[0m         input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1642\u001b[0m         expand_size\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_return_sequences,\n\u001b[1;32m   1643\u001b[0m         is_encoder_decoder\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mis_encoder_decoder,\n\u001b[1;32m   1644\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m   1645\u001b[0m     )\n\u001b[1;32m   1647\u001b[0m     \u001b[39m# 13. run sample\u001b[39;00m\n\u001b[0;32m-> 1648\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msample(\n\u001b[1;32m   1649\u001b[0m         input_ids,\n\u001b[1;32m   1650\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1651\u001b[0m         logits_warper\u001b[39m=\u001b[39;49mlogits_warper,\n\u001b[1;32m   1652\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1653\u001b[0m         pad_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mpad_token_id,\n\u001b[1;32m   1654\u001b[0m         eos_token_id\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49meos_token_id,\n\u001b[1;32m   1655\u001b[0m         output_scores\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49moutput_scores,\n\u001b[1;32m   1656\u001b[0m         return_dict_in_generate\u001b[39m=\u001b[39;49mgeneration_config\u001b[39m.\u001b[39;49mreturn_dict_in_generate,\n\u001b[1;32m   1657\u001b[0m         synced_gpus\u001b[39m=\u001b[39;49msynced_gpus,\n\u001b[1;32m   1658\u001b[0m         streamer\u001b[39m=\u001b[39;49mstreamer,\n\u001b[1;32m   1659\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_kwargs,\n\u001b[1;32m   1660\u001b[0m     )\n\u001b[1;32m   1662\u001b[0m \u001b[39melif\u001b[39;00m generation_mode \u001b[39m==\u001b[39m GenerationMode\u001b[39m.\u001b[39mBEAM_SEARCH:\n\u001b[1;32m   1663\u001b[0m     \u001b[39m# 11. prepare beam search scorer\u001b[39;00m\n\u001b[1;32m   1664\u001b[0m     beam_scorer \u001b[39m=\u001b[39m BeamSearchScorer(\n\u001b[1;32m   1665\u001b[0m         batch_size\u001b[39m=\u001b[39mbatch_size,\n\u001b[1;32m   1666\u001b[0m         num_beams\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mnum_beams,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1671\u001b[0m         max_length\u001b[39m=\u001b[39mgeneration_config\u001b[39m.\u001b[39mmax_length,\n\u001b[1;32m   1672\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/generation/utils.py:2730\u001b[0m, in \u001b[0;36mGenerationMixin.sample\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, logits_warper, max_length, pad_token_id, eos_token_id, output_attentions, output_hidden_states, output_scores, return_dict_in_generate, synced_gpus, streamer, **model_kwargs)\u001b[0m\n\u001b[1;32m   2727\u001b[0m model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprepare_inputs_for_generation(input_ids, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs)\n\u001b[1;32m   2729\u001b[0m \u001b[39m# forward pass to get next token\u001b[39;00m\n\u001b[0;32m-> 2730\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m(\n\u001b[1;32m   2731\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs,\n\u001b[1;32m   2732\u001b[0m     return_dict\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m   2733\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   2734\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   2735\u001b[0m )\n\u001b[1;32m   2737\u001b[0m \u001b[39mif\u001b[39;00m synced_gpus \u001b[39mand\u001b[39;00m this_peer_finished:\n\u001b[1;32m   2738\u001b[0m     \u001b[39mcontinue\u001b[39;00m  \u001b[39m# don't waste resources running the code we don't need\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:165\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    163\u001b[0m         output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    164\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 165\u001b[0m     output \u001b[39m=\u001b[39m old_forward(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[39mreturn\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mpost_forward(module, output)\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:820\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    817\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m    819\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m--> 820\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m    821\u001b[0m     input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    822\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    823\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    824\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m    825\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    826\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    827\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    828\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    829\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    830\u001b[0m )\n\u001b[1;32m    832\u001b[0m hidden_states \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    833\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpretraining_tp \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:708\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    701\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    702\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m    703\u001b[0m         hidden_states,\n\u001b[1;32m    704\u001b[0m         attention_mask,\n\u001b[1;32m    705\u001b[0m         position_ids,\n\u001b[1;32m    706\u001b[0m     )\n\u001b[1;32m    707\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 708\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m    709\u001b[0m         hidden_states,\n\u001b[1;32m    710\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    711\u001b[0m         position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[1;32m    712\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m    713\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    714\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m    715\u001b[0m     )\n\u001b[1;32m    717\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    719\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:160\u001b[0m, in \u001b[0;36madd_hook_to_module.<locals>.new_forward\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(old_forward)\n\u001b[1;32m    159\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnew_forward\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 160\u001b[0m     args, kwargs \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39;49m_hf_hook\u001b[39m.\u001b[39;49mpre_forward(module, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    161\u001b[0m     \u001b[39mif\u001b[39;00m module\u001b[39m.\u001b[39m_hf_hook\u001b[39m.\u001b[39mno_grad:\n\u001b[1;32m    162\u001b[0m         \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/hooks.py:290\u001b[0m, in \u001b[0;36mAlignDevicesHook.pre_forward\u001b[0;34m(self, module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    285\u001b[0m                 fp16_statistics \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_map[name\u001b[39m.\u001b[39mreplace(\u001b[39m\"\u001b[39m\u001b[39mweight\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mSCB\u001b[39m\u001b[39m\"\u001b[39m)]\n\u001b[1;32m    286\u001b[0m         set_module_tensor_to_device(\n\u001b[1;32m    287\u001b[0m             module, name, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecution_device, value\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mweights_map[name], fp16_statistics\u001b[39m=\u001b[39mfp16_statistics\n\u001b[1;32m    288\u001b[0m         )\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m send_to_device(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mexecution_device), send_to_device(\n\u001b[1;32m    291\u001b[0m     kwargs, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecution_device, skip_keys\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mskip_keys\n\u001b[1;32m    292\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/utils/operations.py:160\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[0;32m--> 160\u001b[0m         {\n\u001b[1;32m    161\u001b[0m             k: t \u001b[39mif\u001b[39;49;00m k \u001b[39min\u001b[39;49;00m skip_keys \u001b[39melse\u001b[39;49;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;49;00m k, t \u001b[39min\u001b[39;49;00m tensor\u001b[39m.\u001b[39;49mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/utils/operations.py:161\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[39melif\u001b[39;00m skip_keys \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m         skip_keys \u001b[39m=\u001b[39m []\n\u001b[1;32m    159\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mtype\u001b[39m(tensor)(\n\u001b[1;32m    160\u001b[0m         {\n\u001b[0;32m--> 161\u001b[0m             k: t \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m skip_keys \u001b[39melse\u001b[39;00m send_to_device(t, device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking, skip_keys\u001b[39m=\u001b[39;49mskip_keys)\n\u001b[1;32m    162\u001b[0m             \u001b[39mfor\u001b[39;00m k, t \u001b[39min\u001b[39;00m tensor\u001b[39m.\u001b[39mitems()\n\u001b[1;32m    163\u001b[0m         }\n\u001b[1;32m    164\u001b[0m     )\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/lams/lib/python3.11/site-packages/accelerate/utils/operations.py:167\u001b[0m, in \u001b[0;36msend_to_device\u001b[0;34m(tensor, device, non_blocking, skip_keys)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mhasattr\u001b[39m(tensor, \u001b[39m\"\u001b[39m\u001b[39mto\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m    166\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 167\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mto(device, non_blocking\u001b[39m=\u001b[39;49mnon_blocking)\n\u001b[1;32m    168\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:  \u001b[39m# .to() doesn't accept non_blocking as kwarg\u001b[39;00m\n\u001b[1;32m    169\u001b[0m         \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "The_Topic = '\"Is India Truely a Democracy?\"'\n",
    "no_users = 4\n",
    "\n",
    "\n",
    "def Background_prototype(user):\n",
    "    return f'You are {user}. The topic of the discussion is {The_Topic}. Ask a question or write a response that answers the question of other people according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "\n",
    "names = [\"user\"+str(i) for i in range(no_users)]\n",
    "Backgrounds = [Background_prototype(user) for user in names]\n",
    "\n",
    "history = ''\n",
    "next_prompt = 'Hello Everyone, Greet yourself and continue conversation.\\n'\n",
    "\n",
    "for i in range(4):\n",
    "    for j in range(no_users):\n",
    "        next_prompt = generate_response(format_prompt(next_prompt, history, Backgrounds[j]), 150, 0.001)\n",
    "        if next_prompt[:len(names[j])] == names[j]:\n",
    "            history += next_prompt + '\\n'\n",
    "        else:\n",
    "            history += f\"{names[j]}: \" + next_prompt + '\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user0:  Sure, I'd be happy to continue the conversation! Here's my response:\n",
      "What are some of the key challenges facing India's democracy today, and how can they be addressed?\n",
      "user1:  One of the major challenges facing India's democracy today is the erosion of federalism and the concentration of power in the hands of the central government, which can be addressed by strengthening the role of state governments and decentralizing power.\n",
      "user2:  Thanks for sharing your thoughts on this topic! Here's my response:\n",
      "What are the potential consequences of this erosion of federalism, and how might it impact India's democratic system in the long term?\n",
      "user3:  Great question! The erosion of federalism in India could lead to a concentration of power in the hands of the central government, which could result in a weakening of democratic institutions and processes at the state level. This could lead to a decline in the quality of governance and the ability of state governments to respond to the unique needs and challenges of their respective regions. In the long term, this could undermine India's democratic system and lead to a decline in the country's overall political stability and economic development.\n",
      "user4:  Thanks for your response! Here's my next question:\n",
      "How does the erosion of federalism in India impact the country's economic development, and what are some potential solutions to address these challenges?\n",
      "user0:  Great question! The erosion of federalism in India can have a significant impact on the country's economic development, as it can lead to a lack of investment in infrastructure and public services in state capitals, which are critical for attracting businesses and promoting economic growth. To address this challenge, some potential solutions could include devolving more powers and resources to state governments, promoting greater coordination and cooperation between the center and states, and investing in infrastructure and public services at the state level.\n",
      "user1:  Thanks for your response! Here's my next question:\n",
      "How does the erosion of federalism in India impact the country's political stability, and what are some potential consequences of this trend?\n",
      "user2:  Great question! The erosion of federalism in India can have significant impacts on the country's political stability, as it can lead to a concentration of power in the hands of the central government, which can undermine the autonomy and representation of state governments and regional interests. This can lead to political tensions and conflicts between the center and states, and can also result in a decline in the quality of governance and the ability of state governments to respond to the unique needs and challenges of their respective regions. In the long term, this could lead to a decline in the country's overall political stability and economic development.\n",
      "user3:  Thanks for your response! Here's my next question:\n",
      "How does the erosion of federalism in India impact the country's ability to address pressing social and economic issues, such as poverty and inequality?\n",
      "user4:  Great question! The erosion of federalism in India can significantly impact the country's ability to address pressing social and economic issues, such as poverty and inequality, as it can lead to a concentration of power in the hands of the central government, which may not be able to fully understand and address the unique needs and challenges of different regions and communities. This can result in a lack of investment in social and economic development programs in state capitals, which are critical for reducing poverty and inequality. Additionally, the centralization of power can lead to a lack of representation and autonomy for marginalized communities, which can further exacerbate existing inequalities.\n",
      "user0:  Thanks for your response! Here's my next question:\n",
      "How might the erosion of federalism in India impact the country's international relations and its position in global affairs?\n",
      "user1:  Great question! The erosion of federalism in India could have significant implications for the country's international relations and position in global affairs. With a concentration of power in the hands of the central government, India may appear less democratic and more authoritarian to other countries, which could impact its ability to form alliances and partnerships. Additionally, the lack of autonomy and representation for state governments could lead to internal divisions and conflicts, which could also impact India's ability to project power and influence globally. In the long term, this could lead to a decline in India's international standing and its ability to play a significant role in global affairs.\n",
      "user2:  Thanks for your response! Here's my next question:\n",
      "How might the erosion of federalism in India impact the country's economic growth and development, and what are some potential solutions to address these challenges?\n",
      "user3:  Great question! The erosion of federalism in India can have significant impacts on the country's economic growth and development, as it can lead to a concentration of power in the hands of the central government, which may not be able to fully understand and address the unique needs and challenges of different regions and industries. This can result in a lack of investment in infrastructure and public services in state capitals, which are critical for attracting businesses and promoting economic growth. Additionally, the centralization of power can lead to a lack of representation and autonomy for marginalized communities, which can further exacerbate existing inequalities.\n",
      "To address these challenges, some potential solutions could include:\n",
      "user4:  Thanks for your response! Here's my next question:\n",
      "How might the erosion of federalism in India impact the country's ability to address pressing social and economic issues, such as poverty and inequality?\n",
      "user0:  Great question! The erosion of federalism in India can significantly impact the country's ability to address pressing social and economic issues, such as poverty and inequality, for several reasons:\n",
      "1. Lack of representation and autonomy for marginalized communities: With a concentration of power in the hands of the central government, marginalized communities may have limited representation and autonomy to advocate for their interests and address their specific needs. This can exacerbate existing inequalities and make it more difficult to reduce poverty and inequality.\n",
      "2. Limited investment in social and economic development programs: The centralization of power can lead to a lack of investment in social and economic development programs in state capitals\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The_Topic = '\"What if India becomes a superpower within next 10 years.\"'\n",
    "# User_A = \"Tom\"\n",
    "# User_B = \"Jerry\"\n",
    " \n",
    "# # Background_for_A = f'You are {User_A}. The topic of the discussion id {The_Topic}. Ask a question or write a response that answers the question of {User_B} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "# # Background_for_B = f'You are {User_B}. The topic of the discussion id {The_Topic}. Ask a question or write a response that answers the question of {User_A} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "\n",
    "# Background_for_A = f'You are {User_A}. The topic of the discussion is Communism. You are in favour of Communism. Ask a question or write a response that answers the question of {User_B} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "# Background_for_B = f'You are {User_B}. The topic of the discussion is Communism. You are completely against the communism. Ask a question or write a response that answers the question of {User_A} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "# Background = ''\n",
    "# the_output = ''\n",
    "\n",
    "# history = f'''\n",
    "# {User_A}: Hello, I am {User_A}\n",
    "# {User_B}:  Hello {User_A}, I am {User_B}\n",
    "# '''\n",
    "# prompt_to_A = f\"Hello {User_A}, I am {User_B}\"\n",
    "\n",
    "# for j in trange(1,30):\n",
    "#     try:\n",
    "#         prompt_to_B = generate_response(format_prompt(prompt_to_A, history, Background_for_A), 150, 0.001)\n",
    "#         if prompt_to_B[:len(User_A)] == User_A:\n",
    "#             history += prompt_to_B + '\\n'\n",
    "#         else:\n",
    "#             history += f\"{User_A}: \" + prompt_to_B + '\\n'\n",
    "\n",
    "#         prompt_to_A = generate_response(format_prompt(prompt_to_B, history, Background_for_B), 150, 0.001)\n",
    "#         if prompt_to_A[:len(User_B)] == User_B:\n",
    "#             history += prompt_to_A + '\\n'\n",
    "#         else:\n",
    "#             history += f\"{User_B}: \" + prompt_to_A + '\\n'\n",
    "        \n",
    "#         if not j % 5:\n",
    "#             summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "#             summary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "#             print(summary)\n",
    "#             print(\"*\"*50)\n",
    "#             history = \"We know that\\n\" + summary +'\\n' + f'''\n",
    "# {User_A}: Hello, I am {User_A}\n",
    "# {User_B}:  Hello {User_A}, I am {User_B}\n",
    "# '''\n",
    "#     except:\n",
    "#         break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "# newsummary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # newsummary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "# # print(newsummary)\n",
    "# print(len(history))\n",
    "# print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### SAVE TO TEXT FILE ########\n",
    "\n",
    "# # Define the string you want to append to the file\n",
    "# text_to_append = \"This is additional text to append to the file.\"\n",
    "\n",
    "# # Specify the file path where you want to append the text\n",
    "# file_path = f\"./{The_Topic}.txt\"\n",
    "# file_path = f\"./AGI_tobeinvented.txt\"\n",
    "\n",
    "# # Open the file in append mode ('a' for appending)\n",
    "# with open(file_path, 'a') as file:\n",
    "#     # Write the string to the file\n",
    "#     file.write(\"Conversation\\n\\n\")\n",
    "#     file.write(history)\n",
    "#     file.write(\"Summary\\n\\n\")\n",
    "#     file.write(summary)\n",
    "\n",
    "# # Close the file when you're done (the 'with' statement automatically does this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_for_A = '''\n",
    "# User A: Hello, I am User A. \n",
    "# User B:  Hello User A, I am User B.\n",
    "# '''\n",
    "\n",
    "# history_for_B = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B: Hello User A, I am User B.\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed_Prefix = \"Write a response that answers the request according to the conversation history below. Answer in one short sentence only.\"\n",
    "# Background_for_A = 'Talk about Fake news with User B'\n",
    "# Background_for_B = 'Talk about Fake news with User A'\n",
    "# Background = ''\n",
    "# the_output = ''\n",
    "# Fixed_Prefix_len = len(Fixed_Prefix)\n",
    "# import sys\n",
    "# history = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B:  Hello!\n",
    "# '''\n",
    "# for j in range(1,50):\n",
    "#     if not j % 5:    \n",
    "#         summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "#         Background += '\\n' + ''.join(Background.strip().split('\\n')[1:])\n",
    "#         history = '\\n'\n",
    "#     print(Background)\n",
    "#     # print(history)\n",
    "#     print(the_output)\n",
    "#     print(j, \"*\" * 50)\n",
    "#     while True:\n",
    "#         prompt = input()\n",
    "#         if len(prompt) == 0:\n",
    "#             continue\n",
    "#         else:\n",
    "#             break\n",
    "#     if prompt == 'quit':\n",
    "#         break\n",
    "#     the_output = generate_response(format_prompt(prompt, history), 200, 0.001)\n",
    "#     history += \"Request: \" + prompt + '\\n'\n",
    "#     history += \"Response: \" + the_output + '\\n\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
