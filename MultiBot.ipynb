{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GenerationConfig, LlamaForCausalLM, LlamaTokenizer\n",
    "import torch\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\n",
    "MODEL_NAME = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "\n",
    "tokenizer = LlamaTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    return_dict=True,\n",
    "    # load_in_8bit=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    ")\n",
    "generation_config = GenerationConfig.from_pretrained(MODEL_NAME)\n",
    "print(generation_config)\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(prompt: str, max_new_tokens: int = 128, temperature: float = 0.001) -> str:\n",
    "    encoding = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
    "    with torch.inference_mode():\n",
    "        outputs = model.generate(\n",
    "            **encoding,\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            temperature=0.001,\n",
    "            generation_config=generation_config,\n",
    "\n",
    "            do_sample = True,\n",
    "        )\n",
    "    answer_tokens = outputs[:, encoding.input_ids.shape[1] :]\n",
    "    return tokenizer.decode(answer_tokens[0], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def format_prompt(prompt, history='', Background=''):\n",
    "    return f\"\"\"<s>[INST] <<SYS>>\n",
    "{{ \n",
    "    {Background}\n",
    "    {history}\n",
    "}}\n",
    "<</SYS>>\n",
    "[/INST]  </s>\n",
    "<s>[INST] {{ {prompt} }} [/INST]\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "The_Topic = '\"What if India becomes a superpower within next 10 years.\"'\n",
    "User_A = \"Tom\"\n",
    "User_B = \"Jerry\"\n",
    " \n",
    "# Background_for_A = f'You are {User_A}. The topic of the discussion id {The_Topic}. Ask a question or write a response that answers the question of {User_B} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "# Background_for_B = f'You are {User_B}. The topic of the discussion id {The_Topic}. Ask a question or write a response that answers the question of {User_A} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "\n",
    "Background_for_A = f'You are {User_A}. The topic of the discussion is Communism. You are in favour of Communism. Ask a question or write a response that answers the question of {User_B} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "Background_for_B = f'You are {User_B}. The topic of the discussion is Communism. You are completely against the communism. Ask a question or write a response that answers the question of {User_A} according to the conversation history, or with a new information. Answer in one short sentence only and strictly, do not repeat what is already said in the conversation. If you have nothing new to add, reply with a thanks.'\n",
    "Background = ''\n",
    "the_output = ''\n",
    "\n",
    "history = f'''\n",
    "{User_A}: Hello, I am {User_A}\n",
    "{User_B}:  Hello {User_A}, I am {User_B}\n",
    "'''\n",
    "prompt_to_A = f\"Hello {User_A}, I am {User_B}\"\n",
    "\n",
    "for j in trange(1,30):\n",
    "    try:\n",
    "        prompt_to_B = generate_response(format_prompt(prompt_to_A, history, Background_for_A), 150, 0.001)\n",
    "        if prompt_to_B[:len(User_A)] == User_A:\n",
    "            history += prompt_to_B + '\\n'\n",
    "        else:\n",
    "            history += f\"{User_A}: \" + prompt_to_B + '\\n'\n",
    "\n",
    "        prompt_to_A = generate_response(format_prompt(prompt_to_B, history, Background_for_B), 150, 0.001)\n",
    "        if prompt_to_A[:len(User_B)] == User_B:\n",
    "            history += prompt_to_A + '\\n'\n",
    "        else:\n",
    "            history += f\"{User_B}: \" + prompt_to_A + '\\n'\n",
    "        \n",
    "        if not j % 5:\n",
    "            summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "            summary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "            print(summary)\n",
    "            print(\"*\"*50)\n",
    "            history = \"We know that\\n\" + summary +'\\n' + f'''\n",
    "{User_A}: Hello, I am {User_A}\n",
    "{User_B}:  Hello {User_A}, I am {User_B}\n",
    "'''\n",
    "    except:\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "# newsummary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "\n",
    "# print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # newsummary = '\\n'.join(summary.strip().split('\\n')[1:])\n",
    "# # print(newsummary)\n",
    "# print(len(history))\n",
    "# print(len(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### SAVE TO TEXT FILE ########\n",
    "\n",
    "# # Define the string you want to append to the file\n",
    "# text_to_append = \"This is additional text to append to the file.\"\n",
    "\n",
    "# # Specify the file path where you want to append the text\n",
    "# file_path = f\"./{The_Topic}.txt\"\n",
    "# file_path = f\"./AGI_tobeinvented.txt\"\n",
    "\n",
    "# # Open the file in append mode ('a' for appending)\n",
    "# with open(file_path, 'a') as file:\n",
    "#     # Write the string to the file\n",
    "#     file.write(\"Conversation\\n\\n\")\n",
    "#     file.write(history)\n",
    "#     file.write(\"Summary\\n\\n\")\n",
    "#     file.write(summary)\n",
    "\n",
    "# # Close the file when you're done (the 'with' statement automatically does this)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history_for_A = '''\n",
    "# User A: Hello, I am User A. \n",
    "# User B:  Hello User A, I am User B.\n",
    "# '''\n",
    "\n",
    "# history_for_B = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B: Hello User A, I am User B.\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixed_Prefix = \"Write a response that answers the request according to the conversation history below. Answer in one short sentence only.\"\n",
    "# Background_for_A = 'Talk about Fake news with User B'\n",
    "# Background_for_B = 'Talk about Fake news with User A'\n",
    "# Background = ''\n",
    "# the_output = ''\n",
    "# Fixed_Prefix_len = len(Fixed_Prefix)\n",
    "# import sys\n",
    "# history = '''\n",
    "# User A: Hello, I am User A.\n",
    "# User B:  Hello!\n",
    "# '''\n",
    "# for j in range(1,50):\n",
    "#     if not j % 5:    \n",
    "#         summary = generate_response(format_prompt(\"What are the keypoints of above chat and summary, in very very short.\", history), 500)\n",
    "#         Background += '\\n' + ''.join(Background.strip().split('\\n')[1:])\n",
    "#         history = '\\n'\n",
    "#     print(Background)\n",
    "#     # print(history)\n",
    "#     print(the_output)\n",
    "#     print(j, \"*\" * 50)\n",
    "#     while True:\n",
    "#         prompt = input()\n",
    "#         if len(prompt) == 0:\n",
    "#             continue\n",
    "#         else:\n",
    "#             break\n",
    "#     if prompt == 'quit':\n",
    "#         break\n",
    "#     the_output = generate_response(format_prompt(prompt, history), 200, 0.001)\n",
    "#     history += \"Request: \" + prompt + '\\n'\n",
    "#     history += \"Response: \" + the_output + '\\n\\n'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lams",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
